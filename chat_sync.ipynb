{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2414799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot.py\n",
    "\n",
    "def greet():\n",
    "    return \"Hello! How can I help you today?\"\n",
    "\n",
    "def respond(user_input):\n",
    "    # Simple rule-based responses\n",
    "    if \"hello\" in user_input.lower() or \"hi\" in user_input.lower():\n",
    "        return \"Hello there!\"\n",
    "    elif \"how are you\" in user_input.lower():\n",
    "        return \"I'm doing well, thank you!\"\n",
    "    elif \"bye\" in user_input.lower() or \"goodbye\" in user_input.lower():\n",
    "        return \"Goodbye! Have a great day.\"\n",
    "    else:\n",
    "        return \"I'm sorry, I don't understand.  Can you rephrase?\"\n",
    "\n",
    "def main():\n",
    "    print(greet())\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        response = respond(user_input)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chat.util import Chat, reflections\n",
    "\n",
    "# Defina pares de padrões e respostas\n",
    "pairs = [\n",
    "    [r'hello|hi', ['Hello there!', 'Hi! How can I assist you?']],\n",
    "    [r'how are you?', ['I am good, thank you!', 'Doing well, and you?']],\n",
    "    [r'bye|goodbye', ['Goodbye! Have a great day!', 'See you later!']],\n",
    "    [r'(.*)', ['I am sorry, I do not understand. Can you rephrase?']]\n",
    "]\n",
    "\n",
    "# Crie um chatbot usando NLTK\n",
    "chatbot = Chat(pairs, reflections)\n",
    "\n",
    "# Inicie o chatbot\n",
    "print('Hello! I am a chatbot. Type exit to end the conversation.')\n",
    "while True:\n",
    "    user_input = input('You: ')\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    response = chatbot.respond(user_input)\n",
    "    print('Bot:', response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b61d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando o chatbot...\n",
      "Chatbot inicializado!\n",
      "Digite 'exit' para sair ou 'reset' para limpar o contexto.\n",
      "\n",
      "Chatbot: Contexto da conversa limpo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm not sure if you're being serious or not, but I'm going to assume you're being serious.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def initialize_dialogpt(model_name=\"microsoft/DialoGPT-medium\"):\n",
    "    \"\"\"\n",
    "    Inicializa o tokenizer e o modelo DialoGPT.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_response(tokenizer, model, chat_history_ids, user_input, max_length=1000):\n",
    "    \"\"\"\n",
    "    Gera uma resposta baseada no histórico da conversa e na entrada do usuário.\n",
    "    \n",
    "    Parâmetros:\n",
    "        tokenizer: O tokenizer do modelo.\n",
    "        model: O modelo de linguagem.\n",
    "        chat_history_ids: Histórico de tokens da conversa.\n",
    "        user_input: A mensagem do usuário.\n",
    "        max_length: Tamanho máximo da resposta.\n",
    "    \n",
    "    Retorna:\n",
    "        response: Resposta do chatbot.\n",
    "        chat_history_ids: Histórico atualizado da conversa.\n",
    "    \"\"\"\n",
    "    # Codifica a entrada do usuário\n",
    "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    \n",
    "    # Concatena o histórico anterior com a nova entrada\n",
    "    bot_input_ids = new_input_ids if chat_history_ids is None else torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
    "    \n",
    "    # Gera a resposta considerando o contexto\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Extrai apenas a parte da resposta gerada (exclui a entrada do usuário)\n",
    "    response_ids = chat_history_ids[:, bot_input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response, chat_history_ids\n",
    "\n",
    "def chat():\n",
    "    \"\"\"\n",
    "    Função principal para interagir com o chatbot.\n",
    "    \n",
    "    Comandos:\n",
    "        \"exit\"  -> Finaliza a conversa.\n",
    "        \"reset\" -> Limpa o contexto da conversa.\n",
    "    \"\"\"\n",
    "    print(\"Inicializando o chatbot...\")\n",
    "    tokenizer, model = initialize_dialogpt()\n",
    "    print(\"Chatbot inicializado!\\nDigite 'exit' para sair ou 'reset' para limpar o contexto.\\n\")\n",
    "    \n",
    "    chat_history_ids = None\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Você: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Chatbot: Até logo!\")\n",
    "            break\n",
    "        if user_input.lower() == \"reset\":\n",
    "            chat_history_ids = None\n",
    "            print(\"Chatbot: Contexto da conversa limpo.\")\n",
    "            continue\n",
    "        \n",
    "        response, chat_history_ids = generate_response(tokenizer, model, chat_history_ids, user_input)\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf87fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando o chatbot...\n",
      "Chatbot inicializado!\n",
      "Digite 'exit' para sair ou 'reset' para limpar o contexto.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm not sure if you're being serious or not, but I'm going to assume you are.\n",
      "Chatbot: Hi, I am not being sarcastic. I have no idea what you mean. What is your favorite color?\n",
      "Chatbot: Oh, that's a good one. Purple is my favorite colour. It's so pretty. And it's not too bright. So pretty!\n",
      "Chatbot: That's my favourite colour too! I love it. :D\n",
      "Chatbot: Yes, yes I do. That's why I like it so much. lt 3\n",
      "Chatbot: Contexto da conversa limpo.\n",
      "Chatbot: I am a Soul.\n",
      "\n",
      "Chatbot: Interação interrompida pelo usuário.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def initialize_dialogpt(model_name=\"microsoft/DialoGPT-medium\"):\n",
    "    \"\"\"\n",
    "    Initializes the tokenizer and the DialoGPT model.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_response(tokenizer, model, chat_history_ids, user_input, max_length=1000):\n",
    "    \"\"\"\n",
    "    Generates a response based on the chat history and user input.\n",
    "    \n",
    "    Parameters:\n",
    "        tokenizer: The tokenizer of the model.\n",
    "        model: The language model.\n",
    "        chat_history_ids: Chat history token IDs.\n",
    "        user_input: User input message.\n",
    "        max_length: Maximum length of the response.\n",
    "    \n",
    "    Returns:\n",
    "        response: Chatbot response.\n",
    "        chat_history_ids: Updated chat history.\n",
    "    \"\"\"\n",
    "    # Encode the user input\n",
    "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    \n",
    "    # Concatenate the previous history with the new input\n",
    "    bot_input_ids = new_input_ids if chat_history_ids is None else torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
    "    \n",
    "    # Generate a response considering the context\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=2)\n",
    "    \n",
    "    # Extract only the part of the response generated (excluding the user input)\n",
    "    response_ids = chat_history_ids[:, bot_input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip(), chat_history_ids\n",
    "\n",
    "def chat():\n",
    "    \"\"\"\n",
    "    Main function to interact with the chatbot.\n",
    "    \n",
    "    Commands:\n",
    "        \"exit\"  -> Ends the conversation.\n",
    "        \"reset\" -> Resets the conversation history.\n",
    "    \"\"\"\n",
    "    print(\"Inicializando o chatbot...\")\n",
    "    tokenizer, model = initialize_dialogpt()\n",
    "    print(\"Chatbot inicializado!\\nDigite 'exit' para sair ou 'reset' para limpar o contexto.\\n\")\n",
    "    \n",
    "    chat_history_ids = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Você: \").strip()\n",
    "            if not user_input:\n",
    "                continue  # Skip empty inputs\n",
    "            \n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"Chatbot: Até logo!\")\n",
    "                break\n",
    "            \n",
    "            if user_input.lower() == \"reset\":\n",
    "                chat_history_ids = None\n",
    "                print(\"Chatbot: Contexto da conversa limpo.\")\n",
    "                continue\n",
    "            \n",
    "            response, chat_history_ids = generate_response(tokenizer, model, chat_history_ids, user_input)\n",
    "            print(\"Chatbot:\", response)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nChatbot: Interação interrompida pelo usuário.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Chatbot: Ops! Ocorreu um erro: {e}\")\n",
    "            chat_history_ids = None  # Reset chat history in case of error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from functools import lru_cache # For caching\n",
    "import gc\n",
    "\n",
    "# --- Configuration ---\n",
    "PDF_PATH = \"WEB-Livro-dos-Espíritos-Guillon.pdf\"\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"gpt2\" # Or try EleutherAI/gpt-neo-125M if you have resources\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. Load and Split PDF ---\n",
    "print(f\"Loading PDF: {PDF_PATH}...\")\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "documents = loader.load()\n",
    "if not documents:\n",
    "    print(f\"Error loading {PDF_PATH}.\")\n",
    "    exit()\n",
    "print(f\"Loaded {len(documents)} pages from PDF.\")\n",
    "\n",
    "print(\"Splitting into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "del documents\n",
    "gc.collect()\n",
    "print(f\"Created {len(texts)} chunks.\")\n",
    "\n",
    "# --- 2. Create Embeddings and Vector Store ---\n",
    "print(f\"Loading embeddings: {EMBEDDING_MODEL_NAME}...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={'device': DEVICE}\n",
    ")\n",
    "print(\"Creating FAISS index...\")\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "del texts\n",
    "del embeddings\n",
    "gc.collect()\n",
    "print(\"Vector store created.\")\n",
    "\n",
    "# --- 3. Load LLM ---\n",
    "print(f\"Loading LLM: {LLM_MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "print(\"LLM loaded.\")\n",
    "\n",
    "# --- 4. Create Pipeline ---\n",
    "print(\"Creating pipeline...\")\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True,\n",
    "    device=0 if DEVICE == \"cuda\" else -1,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(\"Pipeline created.\")\n",
    "\n",
    "# --- 5. Caching & Chat History ---\n",
    "# Simple dictionary cache (you could replace with LRU cache)\n",
    "query_cache = {} #query : result\n",
    "\n",
    "# Chat History (Optional, but helps for follow-up questions)\n",
    "chat_history = [] #list of tuples (question, answer)\n",
    "\n",
    "def generate_response(query, conversation_history=None, use_cache=True):\n",
    "    \"\"\"Generates a response, checking cache and incorporating chat history.\"\"\"\n",
    "    # --- 5a. Check Cache ---\n",
    "    if use_cache and query in query_cache:\n",
    "        print(\"Using cached result...\")\n",
    "        return query_cache[query]\n",
    "\n",
    "    # --- 5b. Incorporate Chat History ---\n",
    "    augmented_query = query # start with the user's query\n",
    "    if conversation_history:\n",
    "        #Add previous turns to the query to give context to the LLM\n",
    "        history_context = \"\\n\".join(f\"User: {q}\\nChatbot: {a}\" for q, a in conversation_history[-2:]) #Last two turns only\n",
    "        augmented_query = f\"{history_context}\\nUser question: {query}\" #add it to the existing query\n",
    "        print(f\"Augmented query with history: {augmented_query}\")\n",
    "\n",
    "    # --- 5c. Retrieval and Generation ---\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "    retrieved_docs = retriever.invoke(augmented_query) #Langchain 0.1.x or later\n",
    "\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    prompt = f\"Context: {context_text}\\nUser: {augmented_query}\\nChatbot:\"\n",
    "\n",
    "    # Generation (use pipeline directly for simplicity)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=1024,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # --- 5d. Update Cache ---\n",
    "    if use_cache:\n",
    "        query_cache[query] = answer\n",
    "\n",
    "    return answer\n",
    "\n",
    "# --- 6. Chat Loop ---\n",
    "print(\"\\n--- Chat with your PDF ---\")\n",
    "print(\"Type 'sair' to exit.\")\n",
    "while True:\n",
    "    query = input(\"Você: \")\n",
    "    if query.lower() == 'sair':\n",
    "        print(\"Até logo!\")\n",
    "        break\n",
    "    if not query.strip():\n",
    "        continue\n",
    "\n",
    "    print(\"Pensando...\")\n",
    "    try:\n",
    "        #Generate a response considering history and cache\n",
    "        answer = generate_response(query, chat_history)\n",
    "        print(\"\\nChatbot:\", answer)\n",
    "\n",
    "        #Update chat history\n",
    "        chat_history.append((query, answer))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5316b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import PyPDF2\n",
    "\n",
    "class OllamaPDFChatbot:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_text = self.load_pdf(pdf_path)\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def load_pdf(self, pdf_path):\n",
    "        # Extract text from PDF and split into chunks (e.g., paragraphs)\n",
    "        text_chunks = []\n",
    "        try:\n",
    "            with open(pdf_path, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                for page in reader.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        # Split by double newlines or paragraphs\n",
    "                        chunks = [chunk.strip() for chunk in text.split('\\n\\n') if chunk.strip()]\n",
    "                        text_chunks.extend(chunks)\n",
    "            return text_chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "            return []\n",
    "\n",
    "    def find_best_context(self, query):\n",
    "        # Context retrieval using embeddings\n",
    "        if not self.pdf_text:\n",
    "            return \"Nenhum texto carregado do PDF.\"\n",
    "        embeddings = self.embedding_model.encode([query] + self.pdf_text)\n",
    "        similarities = util.pytorch_cos_sim(embeddings[0], embeddings[1:])\n",
    "        best_match_idx = similarities.argmax()\n",
    "        return self.pdf_text[best_match_idx]\n",
    "\n",
    "    def chat(self):\n",
    "        while True:\n",
    "            query = input(\"Você: \")\n",
    "            if query.lower() == 'sair':\n",
    "                break\n",
    "\n",
    "            context = self.find_best_context(query)\n",
    "\n",
    "            # Use Ollama for generation\n",
    "            response = ollama.chat(\n",
    "                model='llama2',\n",
    "                messages=[\n",
    "                    {'role': 'system', 'content': 'You are a helpful assistant answering questions about a book.'},\n",
    "                    {'role': 'user', 'content': f'Context: {context}\\nQuestion: {query}'}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            print(\"Chatbot:\", response['message']['content'])\n",
    "\n",
    "# Usage:\n",
    "# bot = OllamaPDFChatbot(\"WEB-Livro-dos-Espíritos-Guillon.pdf\")\n",
    "# bot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52b8a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Bem-vindo! I'm here to help you with any questions you may have about \"O Livro dos Espíritos\" (The Book of Spirits) by Carlos Castaneda. Please go ahead and ask your question, and I'll do my best to provide a helpful answer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m bot = OllamaPDFChatbot(\u001b[33m\"\u001b[39m\u001b[33m/Users/ds/Documents/PromptEng/pdfs/WEB-Livro-dos-Espíritos-Guillon.pdf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mbot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mOllamaPDFChatbot.chat\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     42\u001b[39m context = \u001b[38;5;28mself\u001b[39m.find_best_context(query)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Use Ollama for generation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllama2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYou are a helpful assistant answering questions about a book.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mContext: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcontext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mQuestion: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquery\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChatbot:\u001b[39m\u001b[33m\"\u001b[39m, response[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/ollama/_client.py:342\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    298\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    299\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    308\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    309\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    311\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/ollama/_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/ollama/_client.py:120\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    119\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     r.raise_for_status()\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PromptEng/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "bot = OllamaPDFChatbot(\"/Users/ds/Documents/PromptEng/pdfs/WEB-Livro-dos-Espíritos-Guillon.pdf\")\n",
    "bot.chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
